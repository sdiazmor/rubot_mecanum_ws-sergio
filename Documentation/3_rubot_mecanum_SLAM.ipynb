{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# rUBot SLAM\n",
    "\n",
    "Using SLAM (short for Simultaneous Localization and Mapping) techniques, you will be able to execute autonomous navigation with our rUBot.\n",
    "\n",
    "SLAM is a technique used in robotics to explore and map an unknown environment while estimating the pose of the robot itself. As it moves all around, it will be acquiring structured information of the surroundings by processing the raw data coming from its sensors.\n",
    "\n",
    "For optimal and easy-to-understand coverage of the topic of SLAM, we will implement a 360ยบ-coverage Laser Distance Sensor (LDS) in the virtual robot.\n",
    "\n",
    "There are low-cost versions of this sensor technology, such as EAI YDLIDAR X4 (available at https://www.robotshop.com/es/es/escaner-laser-360-ydlidar-x4.html), which is the one we will make use on our rUBot."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Install ROS navigation & SLAM packages\n",
    "First, let's prepare your machine with the required ROS packages needed for the navigation stack (http://wiki.ros.org/navigation):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt-get install ros-melodic-navigation"
   ]
  },
  {
   "source": [
    "And finally the slam_gmapping package, that is already available in its binary version (https://wiki.ros.org/slam_gmapping):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt-get install ros-melodic-slam-gmapping"
   ]
  },
  {
   "source": [
    "Open the .bashrc file and verify the ROS_IP and ROS_MASTER_URI environment variables and source to the proper workspace:\n",
    "\n",
    "source ~/rUBotCoop_LabProject/devel/setup.bash\n",
    "\n",
    "export ROS_IP=192.168.18.83\n",
    "\n",
    "export ROS_MASTER_URI=http://192.168.18.83:11311"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### What do you need to perform robot navigation with ROS?\n",
    "- Mapping: First, you need a map\n",
    "- Localization: Next you need to localize the robot on that map\n",
    "- Path Planning: Now you can send goal locations to the robot\n",
    "- Obstacle avoidance: Finally, you need to avoid obstacles"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First of all we create a \"mecanum_slam\" package with dependencies: roscpp std_msgs sensor_msgs geometry_msgs nav_msgs tf gazebo_ros"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catkin_create_pkg rubotcoop_slam rospy std_msgs sensor_msgs geometry_msgs nav_msgs"
   ]
  },
  {
   "source": [
    "Let's follow these steps to build the map of a simple Gazebo world called square.world:\n",
    "\n",
    "- Launch the robot model within a modeled environment by running the following line of code:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch rubotcoop_slam rubot_world.launch world:=world1.world"
   ]
  },
  {
   "source": [
    "- Launch the SLAM mapping ROS package, including an RViz visualization that superimposes the virtual model of the robot with the actual scan data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch rubotcoop_slam rubot_slam.launch"
   ]
  },
  {
   "source": [
    "Teleoperate the robot to make it cover as much as possible of the surface of the current Gazebo world. Let's do this as usual with the teleoperation package:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rosrun key_teleop key_teleop.py /key_vel:=/cmd_vel\n",
    "or\n",
    "rosrun teleop_twist_keyboard teleop_twist_keyboard.py"
   ]
  },
  {
   "source": [
    "<img src=\"./Images/4_rubot_slam_gmapping.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "As you move the robot, the LDS sensor will acquire scan data from the unknown areas, and you will receive feedback in the RViz window.\n",
    "\n",
    "- Once you've finished the exploration, save the map, generating two files of the formats indicated in the preceding SLAM process subsection, that is, .pgm and .yaml"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rosrun map_server map_saver -f /media/sf_github_manelpuig/rUBot_mecanum_ws/src/robot_slam/nexus_slam/maps/map_world1"
   ]
  },
  {
   "source": [
    "You will get two files in the root folder of your workspace: map_stage_2.pgm and map_stage_2.yaml.\n",
    "\n",
    "Provided with the map, we are ready to perform robot navigation with the rubot"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Driving along a planned trajectory using navigation\n",
    "First, close the \"rubot_slam.launch\" terminal. Then, as in the SLAM process, let's proceed step by step to perform some navigation:\n",
    "- Set up the navigation algorithm and launch RViz. We will use AMCL, the most common choice for effective navigation.\n",
    "\n",
    "In this step, we also provide the costmap that the robot built before. To do this, you just have to reference the .yaml map file you created before. Make sure that the corresponding .pgm file has the same name and is placed in the same location."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch nexus_slam rubot_navigation.launch map_file:=/media/sf_github_manelpuig/rUBot_mecanum_ws/src/robot_slam/nexus_slam/maps/map_world1.yaml"
   ]
  },
  {
   "source": [
    "- The RViz window, shown in the following screenshot, lets you visualize the robot in the environment and mark the target pose (position and orientation) that it should achieve: First of all, you have to tell the robot that this is the initial pose by pressing the 2D Pose Estimate button. Then, mark it on screen (in this particular case, it isn't necessary, since the initial pose is the same as the one the robot had when it started to build the map)\n",
    "\n",
    "Afterward, you can press 2D Nav Goal button and set the target to the bottom-left corner by clicking the left mouse button. Release the mouse when the arrow has the desired orientation. After releasing, the robot will compute the path to follow and start navigating autonomously.\n",
    "\n",
    "The orientation of the red arrow tells the GoPiGo3 in what direction it should stay facing once it has arrived at the target, and the curved line going from the robot to the target is the planned path. Since it has a map of the environment available, the robot is able to plan a path that avoids the obstacles.\n",
    "\n",
    "The blue square around the robot represents the local window for obstacle avoidance planning. This is used by the Dynamic Window Approach (DWA) method, which generates a local path that efficiently evades the obstacles. The DWA method performs the calculations taking into account the robot's dynamics, in particular, its limited velocity and acceleration."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src=\"./Images/4_nexus_slam1.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src=\"./Images/4_nexus_slam2.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}